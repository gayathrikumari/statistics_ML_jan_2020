{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "     active environment : base\n",
      "    active env location : /Users/manishanker.talusani/opt/anaconda3\n",
      "            shell level : 1\n",
      "       user config file : /Users/manishanker.talusani/.condarc\n",
      " populated config files : /Users/manishanker.talusani/.condarc\n",
      "          conda version : 4.8.3\n",
      "    conda-build version : 3.18.9\n",
      "         python version : 3.7.4.final.0\n",
      "       virtual packages : __osx=10.15\n",
      "       base environment : /Users/manishanker.talusani/opt/anaconda3  (writable)\n",
      "           channel URLs : https://conda.anaconda.org/conda-forge/osx-64\n",
      "                          https://conda.anaconda.org/conda-forge/noarch\n",
      "                          https://repo.anaconda.com/pkgs/main/osx-64\n",
      "                          https://repo.anaconda.com/pkgs/main/noarch\n",
      "                          https://repo.anaconda.com/pkgs/r/osx-64\n",
      "                          https://repo.anaconda.com/pkgs/r/noarch\n",
      "          package cache : /Users/manishanker.talusani/opt/anaconda3/pkgs\n",
      "                          /Users/manishanker.talusani/.conda/pkgs\n",
      "       envs directories : /Users/manishanker.talusani/opt/anaconda3/envs\n",
      "                          /Users/manishanker.talusani/.conda/envs\n",
      "               platform : osx-64\n",
      "             user-agent : conda/4.8.3 requests/2.22.0 CPython/3.7.4 Darwin/19.0.0 OSX/10.15\n",
      "                UID:GID : 501:20\n",
      "             netrc file : None\n",
      "           offline mode : False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install -c conda-forge spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en_core_web_sm==2.1.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz (11.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.1 MB 356 kB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: en-core-web-sm\n",
      "  Building wheel for en-core-web-sm (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for en-core-web-sm: filename=en_core_web_sm-2.1.0-py3-none-any.whl size=11074433 sha256=a9bcdda94177823f2fe3df73bedf24ec74b4d9f67a43d554f86dff7f9016b63a\n",
      "  Stored in directory: /private/var/folders/tc/6th0bb7n6q5027lzkd1fg3k00000gn/T/pip-ephem-wheel-cache-ew95g1oh/wheels/59/4f/8c/0dbaab09a776d1fa3740e9465078bfd903cc22f3985382b496\n",
      "Successfully built en-core-web-sm\n",
      "Installing collected packages: en-core-web-sm\n",
      "  Attempting uninstall: en-core-web-sm\n",
      "    Found existing installation: en-core-web-sm 2.2.0\n",
      "    Uninstalling en-core-web-sm-2.2.0:\n",
      "      Successfully uninstalled en-core-web-sm-2.2.0\n",
      "Successfully installed en-core-web-sm-2.1.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['When', 'learning', 'data', 'science', ',', 'you', 'should', \"n't\", 'get', 'discouraged', '!', '\\n', 'Challenges', 'and', 'setbacks', 'are', \"n't\", 'failures', ',', 'they', \"'re\", 'just', 'part', 'of', 'the', 'journey', '.', 'You', \"'ve\", 'got', 'this', '!']\n"
     ]
    }
   ],
   "source": [
    "# Word tokenization\n",
    "from spacy.lang.en import English\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "nlp = English()\n",
    "\n",
    "text = \"\"\"When learning data science, you shouldn't get discouraged!\n",
    "Challenges and setbacks aren't failures, they're just part of the journey. You've got this!\"\"\"\n",
    "\n",
    "#  \"nlp\" Object is used to create documents with linguistic annotations.\n",
    "my_doc = nlp(text)\n",
    "\n",
    "# Create list of word tokens\n",
    "token_list = []\n",
    "for token in my_doc:\n",
    "    token_list.append(token.text)\n",
    "print(token_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"When learning data science, you shouldn't get discouraged!\", \"\\nChallenges and setbacks aren't failures, they're just part of the journey.\", \"You've got this!\"]\n"
     ]
    }
   ],
   "source": [
    "# sentence tokenization\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "nlp = English()\n",
    "\n",
    "# Create the pipeline 'sentencizer' component\n",
    "sbd = nlp.create_pipe('sentencizer')\n",
    "\n",
    "# Add the component to the pipeline\n",
    "nlp.add_pipe(sbd)\n",
    "\n",
    "text = \"\"\"When learning data science, you shouldn't get discouraged!\n",
    "Challenges and setbacks aren't failures, they're just part of the journey. You've got this!\"\"\"\n",
    "\n",
    "#  \"nlp\" Object is used to create documents with linguistic annotations.\n",
    "doc = nlp(text)\n",
    "\n",
    "# create list of sentence tokens\n",
    "sents_list = []\n",
    "for sent in doc.sents:\n",
    "    sents_list.append(sent.text)\n",
    "print(sents_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop word removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stop words: 326\n",
      "First ten stop words: ['was', 'to', 'five', 'hereafter', 'whereafter', 'she', 'now', 'those', 'others', 'throughout', 'became', 'did', 'nor', 'him', 'always', 'nobody', 'ourselves', 'eleven', 'alone', 'ca']\n"
     ]
    }
   ],
   "source": [
    "#Stop words\n",
    "#importing stop words from English language.\n",
    "import spacy\n",
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "#Printing the total number of stop words:\n",
    "print('Number of stop words: %d' % len(spacy_stopwords))\n",
    "\n",
    "#Printing first ten stop words:\n",
    "print('First ten stop words: %s' % list(spacy_stopwords)[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Sentence: [learning, data, science, ,, discouraged, !, \n",
      ", Challenges, setbacks, failures, ,, journey, ., got, !]\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "#Implementation of stop words:\n",
    "filtered_sent=[]\n",
    "\n",
    "#  \"nlp\" Object is used to create documents with linguistic annotations.\n",
    "doc = nlp(text)\n",
    "\n",
    "# filtering stop words\n",
    "for word in doc:\n",
    "    if word.is_stop==False:\n",
    "        filtered_sent.append(word)\n",
    "print(\"Filtered Sentence:\",filtered_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run run\n",
      "runs run\n",
      "running run\n",
      "runner runner\n"
     ]
    }
   ],
   "source": [
    "# Implementing lemmatization\n",
    "lem = nlp(\"run runs running runner\")\n",
    "# finding lemma for each word\n",
    "for word in lem:\n",
    "    print(word.text,word.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part of Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All DET\n",
      "is VERB\n",
      "well ADV\n",
      "that DET\n",
      "ends VERB\n",
      "well ADV\n",
      ". PUNCT\n"
     ]
    }
   ],
   "source": [
    "# POS tagging\n",
    "\n",
    "# importing the model en_core_web_sm of English for vocabluary, syntax & entities\n",
    "import en_core_web_sm\n",
    "\n",
    "# load en_core_web_sm of English for vocabluary, syntax & entities\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "#  \"nlp\" Objectis used to create documents with linguistic annotations.\n",
    "docs = nlp(u\"All is well that ends well.\")\n",
    "\n",
    "for word in docs:\n",
    "    print(word.text,word.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter hate speech modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /Users/manishanker.talusani/opt/anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - nltk\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    certifi-2020.4.5.1         |           py37_0         159 KB  anaconda\n",
      "    nltk-3.4.5                 |           py37_0         2.1 MB  anaconda\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         2.3 MB\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  openssl            conda-forge::openssl-1.1.1f-h0b31af3_0 --> anaconda::openssl-1.1.1-h1de35cc_0\n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  ca-certificates    conda-forge::ca-certificates-2020.4.5~ --> anaconda::ca-certificates-2020.1.1-0\n",
      "  certifi            conda-forge::certifi-2020.4.5.1-py37h~ --> anaconda::certifi-2020.4.5.1-py37_0\n",
      "  conda              conda-forge::conda-4.8.3-py37hc8dfbb8~ --> anaconda::conda-4.8.3-py37_0\n",
      "  nltk                                            pkgs/main --> anaconda\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "certifi-2020.4.5.1   | 159 KB    | ##################################### | 100% \n",
      "nltk-3.4.5           | 2.1 MB    | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n"
     ]
    }
   ],
   "source": [
    "!conda install -c anaconda nltk -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "data = pd.read_csv(\"/Users/manishanker.talusani/Downloads/twitter-sentiment-analysis-hatred-speech/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = data.tweet[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' @user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run',\n",
       " \"@user @user thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.    #disapointed #getthanked\",\n",
       " '  bihday your majesty',\n",
       " '#model   i love u take with u all the time in urð\\x9f\\x93±!!! ð\\x9f\\x98\\x99ð\\x9f\\x98\\x8eð\\x9f\\x91\\x84ð\\x9f\\x91\\x85ð\\x9f\\x92¦ð\\x9f\\x92¦ð\\x9f\\x92¦  ',\n",
       " ' factsguide: society now    #motivation']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Cleaning Tweets \"\"\"\n",
    "tweets = tweets.str.lower()# removing special characters and numbers\n",
    "\n",
    "tweets = tweets.apply(lambda x : re.sub(\"[^a-z\\s]\",\"\",x) )# removing stopwords\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords = set(stopwords.words(\"english\"))\n",
    "\n",
    "tweets = tweets.apply(lambda x : \" \".join(word for word in x.split() if word not in stopwords ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['user father dysfunctional selfish drags kids dysfunction run',\n",
       " 'user user thanks lyft credit cant use cause dont offer wheelchair vans pdx disapointed getthanked',\n",
       " 'bihday majesty',\n",
       " 'model love u take u time ur',\n",
       " 'factsguide society motivation']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document :  user father dysfunctional selfish drags kids dysfunction run\n",
      "Tokens : \n",
      "user\n",
      "father\n",
      "dysfunctional\n",
      "selfish\n",
      "drags\n",
      "kids\n",
      "dysfunction\n",
      "run\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import en_core_web_sm\n",
    "import numpy as np\n",
    "\n",
    "nlp = en_core_web_sm.load()\n",
    "document = nlp(tweets[0])\n",
    "print(\"Document : \",document)\n",
    "print(\"Tokens : \")\n",
    "\n",
    "for token in document:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get word vectors out of word from spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = nlp(tweets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user father dysfunctional selfish drags kids dysfunction run\n"
     ]
    }
   ],
   "source": [
    "print(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user (96,)\n",
      "father (96,)\n",
      "dysfunctional (96,)\n",
      "selfish (96,)\n",
      "drags (96,)\n",
      "kids (96,)\n",
      "dysfunction (96,)\n",
      "run (96,)\n"
     ]
    }
   ],
   "source": [
    "for token in document:\n",
    "    print(token.text, token.vector.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### “token.vector “ creates a vector of size (96,). The above code was to get vector out of every single word, of a single sentence/document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 96)\n"
     ]
    }
   ],
   "source": [
    "document = nlp.pipe(tweets)\n",
    "tweets_vector = np.array([tweet.vector for tweet in document])\n",
    "print(tweets_vector.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take the complete dataset and apply Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = data.tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31962, 96)\n"
     ]
    }
   ],
   "source": [
    "document = nlp.pipe(tweets)\n",
    "tweets_vector = np.array([tweet.vector for tweet in document])\n",
    "print(tweets_vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31962,)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/manishanker.talusani/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data is : 92.91\n",
      "Accuracy on train data is : 92.95\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X = tweets_vector\n",
    "\n",
    "y = data[\"label\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.3, random_state=0)\n",
    "\n",
    "model = LogisticRegression(C=0.1)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"Accuracy on test data is : %0.2f\" %(accuracy_score(y_test, y_pred)*100))\n",
    "\n",
    "y_train_pred = model.predict(X_train)\n",
    "\n",
    "print(\"Accuracy on train data is : %0.2f\" %(accuracy_score(y_train, y_train_pred)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
